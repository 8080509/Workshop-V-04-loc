{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abb2c0d-23c3-4f84-9c6b-14e47ffbae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecc0c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_nr(data, test_size): \n",
    "    split_index = data.shape[0] - test_size\n",
    "    train = data.iloc[:split_index]\n",
    "    test = data.iloc[split_index :]\n",
    "    return train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b18f0d37-3d08-4e9a-a832-bc20d71219de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies walk forward validation for XGBoost Regressor\n",
    "# Inputs: train, test: pandas dataframes of training and testing datasets\n",
    "#         n_estimators_list: a list of integers where the entries are the number of trees\n",
    "#         max_depth_list: a list of integers where the entries are the max_depth\n",
    "#         scoring: a string, this is the type of scoring, can be \"mean_absolute_error\" or \"root_mean_square_error\"\n",
    "# Return value: a list of two rows with each row representing n_estimator and max_depth\n",
    "#               another list of two rows with each row representing train_error and test_error\n",
    "\n",
    "def xgb_walk_forward_validation(data, test_size = 10, fold = 3, params = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.05, 0.1], 'n_estimators' : [100, 200, 300]}, scoring = \"mean_absolute_error\"): \n",
    "    assert test_size <= np.floor(data.shape[0] / fold)\n",
    "    errors = {}\n",
    "    # number of data points in each sample\n",
    "    data_size = np.int64(np.floor(data.shape[0] / fold))\n",
    "    for f in range(fold): \n",
    "        #break the f-th sample taken from the data set and split it into training and testing datasets\n",
    "        sample = data.iloc[f*data_size: (f+1)*data_size]\n",
    "        train, test = train_test_split_nr(sample, test_size)\n",
    "        print(test.shape)\n",
    "\n",
    "        # loop over the parameters\n",
    "        for n in params['n_estimators']: \n",
    "            for depth in params['max_depth']: \n",
    "                for rate in params['learning_rate']: \n",
    "                    predictions = []\n",
    "                    # walk forward validation\n",
    "                    for i in range(test_size):\n",
    "                        \n",
    "                        # split train, test rows into input and output columns\n",
    "                        X_train, y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "                        X_test = pd.DataFrame(test.iloc[i, :-1]).T\n",
    "                        # fit model on X_train and make a prediction\n",
    "                        XGBoost = XGBRegressor(objective='reg:squarederror', learning_rate=rate, n_estimators=n, max_depth=depth)\n",
    "                        XGBoost.fit(X_train, y_train)\n",
    "                        \n",
    "                        #y_test_hat = XGBoost.predict(np.array(X_test).reshape((1,4)))\n",
    "                        y_hat = XGBoost.predict(X_test)\n",
    "                            \n",
    "                        # store forecast in array of predictions\n",
    "                        predictions.append(y_hat)\n",
    "\n",
    "                        #add current observation to the train data for the next loop\n",
    "                        pd.concat([train, pd.DataFrame(test.iloc[i]).T], axis = 'index')\n",
    "\n",
    "        # estimate prediction error\n",
    "        if scoring == \"mean_absolute_error\": \n",
    "            error = mean_absolute_error(test[:, -1], predictions)\n",
    "        elif scoring == \"root_mean_square_error\": \n",
    "            error = np.sqrt(mean_squared_error(test[:, -1], predictions))\n",
    "\n",
    "        errors[[n, depth, rate]] = error\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9890b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04548068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4228b3a-2cd6-4e27-a731-a3b6e5640667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test errors for different model complexities\n",
    "def Plot3D_train_test_errors(params, errors, scoring = 'mean_absolute_error'): \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    plt.axes(ax)\n",
    "    if scoring == 'mean_absolute_error': \n",
    "        ax.set(xlabel = 'Number of Trees', ylabel = 'Maximum Depth of Trees', zlable = 'Mean Absolute Error', title = 'Mean Absolute Error vs Parameters')\n",
    "    elif scoring == 'root_mean_square_error': \n",
    "        ax.set(xlabel = 'Number of Trees', ylabel = 'Maximum Depth of Trees', zlable = 'Root Mean Square Error', title = 'Root Mean Square Error vs Parameters')\n",
    "    ax.scatter(params[0], params[1], errors[0], lbl = 'Training Error')\n",
    "    ax.scatter(params[0], params[1], errors[1], lbl = 'Testing Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def Plot2D_train_test_errors(param, errors, scoring = 'mean_absolute_error'): \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a991973-8ac4-4471-8926-e69429be2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Replace the below file name for your layout if necessary\n",
    "train_raw = pd.read_csv('data/train.csv')\n",
    "train = train_raw # Todo:  Add split here\n",
    "\n",
    "# date_time conversion\n",
    "dates_dt = pd.to_datetime(train['date'])\n",
    "dates_dt_min = dates_dt.min()\n",
    "\n",
    "families = train['family'].unique()\n",
    "assert families.shape == (33,)\n",
    "fam_index = pd.DataFrame(data = np.arange(33), index = families)[0]\n",
    "\n",
    "# Here's the conversion\n",
    "train_txf = train.copy()\n",
    "train_txf['date'] = (pd.to_datetime(train['date']) - dates_dt_min).dt.days\n",
    "train_txf['store_nbr'] = train['store_nbr'] - 1\n",
    "train_txf['family'] = train['family'].apply(fam_index.get)\n",
    "\n",
    "# Allocate sales array\n",
    "sales_shape = (1688, 54, 33)\n",
    "sales = np.zeros(dtype = np.float64, shape = sales_shape)\n",
    "\n",
    "# Fill in the sales array\n",
    "for row in train_txf.itertuples():\n",
    "    day = row.date\n",
    "    store = row.store_nbr\n",
    "    fam = row.family\n",
    "    sales[day, store, fam] = row.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c206a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txf_sample = train_txf.copy()\n",
    "train_txf_swap = train_txf.copy()\n",
    "del train_txf_swap['id']\n",
    "del train_txf_swap['sales']\n",
    "train_txf_swap['sales'] = train_txf['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4eed989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m xgb_walk_forward_validation(train_txf_swap)\n",
      "Cell \u001b[1;32mIn[37], line 33\u001b[0m, in \u001b[0;36mxgb_walk_forward_validation\u001b[1;34m(data, test_size, fold, params, scoring)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# fit model on X_train and make a prediction\u001b[39;00m\n\u001b[0;32m     32\u001b[0m XGBoost \u001b[38;5;241m=\u001b[39m XGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mrate, n_estimators\u001b[38;5;241m=\u001b[39mn, max_depth\u001b[38;5;241m=\u001b[39mdepth)\n\u001b[1;32m---> 33\u001b[0m XGBoost\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#y_test_hat = XGBoost.predict(np.array(X_test).reshape((1,4)))\u001b[39;00m\n\u001b[0;32m     36\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m XGBoost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\mohao\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mohao\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1109\u001b[0m     params,\n\u001b[0;32m   1110\u001b[0m     train_dmatrix,\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1112\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1113\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1114\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1115\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1116\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1117\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1118\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mohao\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mohao\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mohao\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = xgb_walk_forward_validation(train_txf_swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "178c5f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
