{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8abb2c0d-23c3-4f84-9c6b-14e47ffbae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2decdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data sets into training and testing data such that the testing data has $test_size$ datapoints\n",
    "# Inputs: X_data, y_data: Pandas dataframes; test_size: an integer less than the number of \n",
    "#         datapoints in $X_data$ or $y_data$\n",
    "# Outputs: X_train, X_test, y_train, y_test: Pandas dataframes\n",
    "def train_test_split_nr(X_data, y_data, test_size): \n",
    "    split_index = data.shape[0] - test_size\n",
    "    X_train = X_data.iloc[:split_index]\n",
    "    X_test = X_data.iloc[split_index :]\n",
    "    y_train = y_data.iloc[:split_index]\n",
    "    y_test = y_data.iloc[split_index :]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b18f0d37-3d08-4e9a-a832-bc20d71219de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in Pandas dataframes $X_data$ and $y_data$ and splits them evenly into $fold$ parts. Each part \n",
    "# will be further splitted into training and testing sets such that the testing set is of size $test_size$. \n",
    "# Applies walk forward validation for XGBoost on the training and testing sets, and computes the error for \n",
    "# all the combinations of parameters.\n",
    "# Inputs: X_data, y_data: Pandas dataframes of\n",
    "#         test_size: an integer that is the size of the testing data\n",
    "#         fold: an integer such that $data$ will be broken evenly into $fold$ pieces \n",
    "#         params: a dictionary of parameters to be tested on. It must have three keys: 'max_depth', 'learning_rate',  \n",
    "#                 and 'n_estimators', with the corresponding value being lists of integers, floats, and integers. \n",
    "#         scoring: a string that is the type of scoring. It can be \"mean_absolute_error\" or \"root_mean_square_error\"\n",
    "# Output: a dictionary whose keys are lists of parameters and values are the corresponding errors of the XGBoost model\n",
    "#         with those parameters\n",
    "\n",
    "def xgb_walk_forward_validation(X_data, y_data, test_size = 5, fold = 3, params = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.05, 0.1], 'n_estimators' : [100, 200, 300]}, scoring = \"mean_absolute_error\"): \n",
    "    assert test_size <= np.floor(data.shape[0] / fold)\n",
    "    errors = {}\n",
    "    # number of data points in each sample\n",
    "    sample_size = np.int64(np.floor(data.shape[0] / fold))\n",
    "    for f in range(fold): \n",
    "        #break the f-th sample taken from the data set and split it into training and testing datasets\n",
    "        X_sample = X_data.iloc[f*sample_size: (f+1)*sample_size]\n",
    "        y_sample = y_data.iloc[f*sample_size: (f+1)*sample_size]\n",
    "        X_train, X_test, y_train, y_test = train_test_split_nr(sample, test_size)\n",
    "\n",
    "        # loop over the parameters\n",
    "        for n in params['n_estimators']: \n",
    "            for depth in params['max_depth']: \n",
    "                for rate in params['learning_rate']: \n",
    "                    predictions = []\n",
    "                    # walk forward validation\n",
    "                    for i in range(test_size):\n",
    "                        \n",
    "                        # fit the XGBRegressor model on X_train, y_train and make a prediction\n",
    "                        XGBoost = XGBRegressor(objective='reg:squarederror', learning_rate=rate, n_estimators=n, max_depth=depth)\n",
    "                        XGBoost.fit(X_train, y_train)\n",
    "                        \n",
    "                        #y_test_hat = XGBoost.predict(np.array(X_test).reshape((1,4)))\n",
    "                        y_hat = XGBoost.predict(pd.DataFrame(X_test.iloc[i]).T)\n",
    "                            \n",
    "                        # store forecast in array of predictions\n",
    "                        predictions.append(y_hat)\n",
    "\n",
    "                        #add current observation to the training data for the next loop\n",
    "                        pd.concat([X_train, pd.DataFrame(X_test.iloc[i]).T], axis = 'index')\n",
    "                        pd.concat([y_train, pd.DataFrame(y_test.iloc[i]).T], axis = 'index')\n",
    "\n",
    "        # estimate prediction error\n",
    "        if scoring == \"mean_absolute_error\": \n",
    "            error = mean_absolute_error(test[:, -1], predictions)\n",
    "        elif scoring == \"root_mean_square_error\": \n",
    "            error = np.sqrt(mean_squared_error(test[:, -1], predictions))\n",
    "\n",
    "        errors[[n, depth, rate]] = error\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4228b3a-2cd6-4e27-a731-a3b6e5640667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test errors for different model complexities\n",
    "def Plot3D_train_test_errors(params, errors, scoring = 'mean_absolute_error'): \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    plt.axes(ax)\n",
    "    if scoring == 'mean_absolute_error': \n",
    "        ax.set(xlabel = 'Number of Trees', ylabel = 'Maximum Depth of Trees', zlable = 'Mean Absolute Error', title = 'Mean Absolute Error vs Parameters')\n",
    "    elif scoring == 'root_mean_square_error': \n",
    "        ax.set(xlabel = 'Number of Trees', ylabel = 'Maximum Depth of Trees', zlable = 'Root Mean Square Error', title = 'Root Mean Square Error vs Parameters')\n",
    "    ax.scatter(params[0], params[1], errors[0], lbl = 'Training Error')\n",
    "    ax.scatter(params[0], params[1], errors[1], lbl = 'Testing Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def Plot2D_train_test_errors(param, errors, scoring = 'mean_absolute_error'): \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
